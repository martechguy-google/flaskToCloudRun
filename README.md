# flaskToCloudRun
This code documents the following steps
- Building a Dockerfile, so that you can containerize a Python workflow
- For Cloud Run, we need a web server which listens on port 80, so we are using a Flask app
- In the Flask App, we are calling a shell script
- Finally, we use the [Google Cloud article](https://cloud.google.com/run/docs/quickstarts/build-and-deploy/deploy-python-service) to push this to Cloud Run

Let's understand why we even need this in the first place, and alternative workflows. You can decide the best course of action for your use case depending on some of these considerations.
## Main Objective
- We have a [scrapy](https://docs.scrapy.org/en/latest/intro/overview.html) repository that fetches data from certain pages, and we want to be able to schedule it
### Considerations
It can be tricky to simply schedule something on Google Cloud. If you have your own local machine, you can use a cron job on Unix or you could use Task Scheduler on Windows. (Actually, both of them are not as simple as they sound - cron job is sometimes tricky because of different user profiles, PATH variables etc and Task Scheduler is not the most intuitive as well, but generally experienced users are familiar enough to tackle this locally so we would not delve too deep into that). If you are using a Cloud Provider though, lets look at some of our options
- You do of course have the option to install a machine (isnt that the very core function of a cloud service provider, you might ask?). However, then you have to manually install a bunch of libraries etc, build out connections, make sure services are accessible and what not. You are welcome to try this path, and its a valid approach. Its called Compute Engine on GCP. I have tried it, and it gave me a lot of headache, so I am not choosing this path
- On that note, instead of a cron job directly, there might be some packages or wrappers like [schedule](https://schedule.readthedocs.io/en/stable/). Again this would need a machine to be available to work. Google Cloud for example provides cloud shell but that's ephemeral and only remains in use for your session. 
- You can use Cloud Functions to incorporate the function, then use Cloud Scheduler to schedule a cron job. I really like this option for simple tasks. However, for large projects with a lot of interdependent files, libraries etc, it can be tricky to set up, case in point this particular repository. 
- Now, we come to one of the options that will work for pretty much any use case. You can containerize the repository into a Docker image, and then create a Google Kubernetes cluster to schedule it. Sorry for throwing a bunch of jargon there - Docker, Container, Kubernetes - what is going on? This might need some explaining
   - You can find the official explanations for Docker on several sites. [Example](https://www.ibm.com/sg-en/topics/docker). Before I used it myself though, I was never quite able to make sense of this language and internalize what this meant. This is how I understand it now. If you want to implement a script like this, you shall obviously need a bunch of libraries, a specific type of environment etc. That is why Compute Engine was so tough, isnt it, because you have to spend a bunch of time installing and configuring things. To oversimplify, what if you could provide a configuration file such that the system could just set up the environment by itself and give you the output on a single click? That is what Docker enables you to do. You have a container which contains language that the system understands to know what stuff to install, execute etc

